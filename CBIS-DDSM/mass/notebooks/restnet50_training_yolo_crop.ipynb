{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fe3ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1 | TorchVision: 0.20.1\n",
      "CUDA available: True | Device: cuda\n",
      "CSV_PATH: C:\\Users\\PC\\Desktop\\final project\\CBIS-DDSM\\mass\\data\\yolo_crops.csv\n",
      "ROOT_DIR: C:\\Users\\PC\\Desktop\\final project\\CBIS-DDSM\\mass\n",
      "Rows (after existence filter): 1574\n",
      "label\n",
      "benign       799\n",
      "malignant    775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albumentations available.\n",
      "Transforms ready.\n",
      "One sample tensor shape: (3, 384, 384)\n",
      "OS: Windows | num_workers=0 | pin_memory=True\n",
      "Train batch: torch.Size([16, 3, 384, 384]) torch.Size([16]) torch.Size([16, 4]) | pos in batch: 9\n",
      "Model output shape: (16, 1)\n",
      "One tiny opt step OK. Loss: 0.4323744475841522\n",
      "Dry evaluation OK — acc=0.619, f1=0.620, auc=0.618\n",
      "Epoch 01: loss=0.4133 | acc=0.5619 | f1=0.5633 | auc=0.5751\n",
      "Epoch 02: loss=0.4039 | acc=0.5905 | f1=0.4735 | auc=0.5829\n",
      "Epoch 03: loss=0.4073 | acc=0.5397 | f1=0.6272 | auc=0.6156\n",
      "Epoch 04: loss=0.4000 | acc=0.5651 | f1=0.5785 | auc=0.6127\n",
      "Epoch 05: loss=0.4021 | acc=0.6127 | f1=0.6090 | auc=0.6477\n",
      "Epoch 06: loss=0.4011 | acc=0.6190 | f1=0.6154 | auc=0.6542\n",
      "Epoch 07: loss=0.3939 | acc=0.5746 | f1=0.5379 | auc=0.5921\n",
      "Epoch 08: loss=0.3809 | acc=0.6159 | f1=0.6388 | auc=0.6710\n",
      "Epoch 09: loss=0.3867 | acc=0.6825 | f1=0.6350 | auc=0.7424\n",
      "Epoch 10: loss=0.3726 | acc=0.6286 | f1=0.5483 | auc=0.6977\n",
      "Epoch 11: loss=0.3731 | acc=0.6571 | f1=0.6538 | auc=0.6796\n",
      "Epoch 12: loss=0.3678 | acc=0.6508 | f1=0.5565 | auc=0.6894\n",
      "Epoch 13: loss=0.3617 | acc=0.6667 | f1=0.6097 | auc=0.7244\n",
      "Epoch 14: loss=0.3483 | acc=0.6381 | f1=0.6481 | auc=0.7225\n",
      "Epoch 15: loss=0.3271 | acc=0.6730 | f1=0.6142 | auc=0.7126\n",
      "Epoch 16: loss=0.3158 | acc=0.6889 | f1=0.6316 | auc=0.7329\n",
      "Epoch 17: loss=0.3275 | acc=0.6698 | f1=0.6886 | auc=0.7448\n",
      "Epoch 18: loss=0.3131 | acc=0.6730 | f1=0.6460 | auc=0.7528\n",
      "Epoch 19: loss=0.3046 | acc=0.6984 | f1=0.6690 | auc=0.7522\n",
      "Epoch 20: loss=0.2940 | acc=0.7206 | f1=0.7047 | auc=0.7756\n",
      "Epoch 21: loss=0.2977 | acc=0.7079 | f1=0.6892 | auc=0.7623\n",
      "Epoch 22: loss=0.2730 | acc=0.6984 | f1=0.6823 | auc=0.7708\n",
      "Epoch 23: loss=0.2776 | acc=0.7270 | f1=0.7133 | auc=0.7721\n",
      "Epoch 24: loss=0.2716 | acc=0.7143 | f1=0.7000 | auc=0.7692\n",
      "Epoch 25: loss=0.2883 | acc=0.7175 | f1=0.7003 | auc=0.7720\n",
      "Best AUC: 0.7756048387096773 | Saved: resnet50_cbis.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_6924\\3746674740.py:345: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(OUT_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ckpt @0.5 — acc=0.6921, f1=0.6689, auc=0.7629\n",
      "\n",
      "=== Metrics at optimal threshold ===\n",
      "Threshold        : 0.329\n",
      "Accuracy         : 0.6667\n",
      "Balanced Acc     : 0.6693\n",
      "Sensitivity      : 0.8323\n",
      "Specificity      : 0.5062\n",
      "Precision        : 0.6202\n",
      "NPV              : 0.7570\n",
      "F1 Score         : 0.7107\n",
      "AUC              : 0.7629\n",
      "MCC              : 0.3573\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 81  79]\n",
      " [ 26 129]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.76      0.51      0.61       160\n",
      "   malignant       0.62      0.83      0.71       155\n",
      "\n",
      "    accuracy                           0.67       315\n",
      "   macro avg       0.69      0.67      0.66       315\n",
      "weighted avg       0.69      0.67      0.66       315\n",
      "\n",
      "Predicted malignant probability: 0.18509960174560547\n",
      "Grad-CAM heatmap shape: (384, 384) min/max: 0.0 0.9999644160270691\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================\n",
    "# Cell 1 — Imports & Configuration\n",
    "# ================================\n",
    "import os, math, random, time, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# ---- Config (edit these two to match your environment) ----\n",
    "CSV_PATH  = r\"C:\\Users\\PC\\Desktop\\final project\\CBIS-DDSM\\mass\\data\\yolo_crops.csv\"  # full path to your CSV\n",
    "ROOT_DIR  = r\"C:\\Users\\PC\\Desktop\\final project\\CBIS-DDSM\\mass\"\n",
    "\n",
    "# Training hyperparameters\n",
    "IMG_SIZE     = 384\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 25\n",
    "LR           = 2e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "OUT_PATH     = \"resnet50_cbis.pt\"   # saved best checkpoint (by AUC)\n",
    "USE_CLAHE    = True                 # set False to disable CLAHE (val too)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| TorchVision:\", torchvision.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available(), \"| Device:\", DEVICE)\n",
    "print(\"CSV_PATH:\", CSV_PATH)\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "assert os.path.exists(CSV_PATH), \"CSV file not found!\"\n",
    "\n",
    "# %%\n",
    "# =====================================\n",
    "# Cell 2 — Load CSV & Basic Sanity Check\n",
    "# =====================================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "required_cols = {\"yolo_cropped_image_path\", \"label\",\n",
    "                 \"laterality_LEFT\",\"laterality_RIGHT\",\"view_CC\",\"view_MLO\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"Missing columns in CSV: {missing}\"\n",
    "\n",
    "def _exists(rel):\n",
    "    p = str(rel).replace(\"\\\\\", os.sep)\n",
    "    if ROOT_DIR and not os.path.isabs(p):\n",
    "        p = os.path.join(ROOT_DIR, p)\n",
    "    return os.path.exists(p)\n",
    "\n",
    "df[\"exists\"] = df[\"yolo_cropped_image_path\"].apply(_exists)\n",
    "df = df[df[\"exists\"]].copy()\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(\"Rows (after existence filter):\", len(df))\n",
    "print(df[\"label\"].value_counts().rename({0:\"benign\",1:\"malignant\"}).to_string())\n",
    "assert len(df) > 0, \"No valid rows after existence filter.\"\n",
    "\n",
    "# %%\n",
    "# ============================\n",
    "# Cell 3 — Transforms (Fixed)\n",
    "# ============================\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    import cv2\n",
    "    _HAS_ALBU = True\n",
    "    print(\"Albumentations available.\")\n",
    "except Exception:\n",
    "    _HAS_ALBU = False\n",
    "    print(\"Albumentations NOT available. Falling back to torchvision transforms.\")\n",
    "\n",
    "def build_transforms(img_size=IMG_SIZE, use_clahe=USE_CLAHE):\n",
    "    if _HAS_ALBU:\n",
    "        BLACK3 = (0, 0, 0)  # required when using BORDER_CONSTANT\n",
    "        aug_train = A.Compose([\n",
    "            (A.CLAHE(clip_limit=3.0, tile_grid_size=(8,8), p=1.0) if use_clahe else A.NoOp()),\n",
    "            A.GaussianBlur(blur_limit=(3,5), p=0.2),\n",
    "            A.MotionBlur(blur_limit=3, p=0.1),\n",
    "            A.RandomBrightnessContrast(0.15, 0.25, p=0.6),\n",
    "            A.RandomResizedCrop(size=(img_size, img_size), scale=(0.9, 1.0), ratio=(0.9, 1.1), p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.10, rotate_limit=20,\n",
    "                               border_mode=cv2.BORDER_CONSTANT, value=BLACK3, p=0.6),\n",
    "            A.Downscale(scale_min=0.90, scale_max=0.95, p=0.2),\n",
    "            A.GaussNoise(var_limit=(5.0, 20.0), p=0.2),\n",
    "            A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        aug_val = A.Compose([\n",
    "            (A.CLAHE(clip_limit=3.0, tile_grid_size=(8,8), p=1.0) if use_clahe else A.NoOp()),\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size,\n",
    "                          border_mode=cv2.BORDER_CONSTANT, value=BLACK3, p=1.0),\n",
    "            A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        aug_train = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ])\n",
    "        aug_val = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ])\n",
    "    return aug_train, aug_val\n",
    "\n",
    "t_train, t_val = build_transforms()\n",
    "print(\"Transforms ready.\")\n",
    "\n",
    "# quick validation\n",
    "sample_path = os.path.join(ROOT_DIR, str(df.iloc[0][\"yolo_cropped_image_path\"]).replace(\"\\\\\", os.sep))\n",
    "img = Image.open(sample_path).convert(\"L\")\n",
    "if _HAS_ALBU:\n",
    "    arr = np.stack([np.array(img)]*3, axis=2).astype(np.uint8)\n",
    "    x = t_val(image=arr)[\"image\"]\n",
    "else:\n",
    "    x = t_val(img)\n",
    "print(\"One sample tensor shape:\", tuple(x.shape))  # (3, IMG_SIZE, IMG_SIZE)\n",
    "assert x.shape[0] == 3 and x.shape[1] == IMG_SIZE and x.shape[2] == IMG_SIZE, \"Transform shape mismatch!\"\n",
    "\n",
    "# %%\n",
    "# ===================================\n",
    "# Cell 4 — Dataset & DataLoaders\n",
    "# ===================================\n",
    "IS_WIN = (os.name == \"nt\") or (\"windows\" in platform.system().lower())\n",
    "NUM_WORKERS = 0 if IS_WIN else 4\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "\n",
    "class CropsDataset(Dataset):\n",
    "    def __init__(self, df, root_dir=ROOT_DIR, transform=None, path_col=\"yolo_cropped_image_path\", label_col=\"label\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root = root_dir\n",
    "        self.t = transform\n",
    "        self.path_col = path_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        rel = str(row[self.path_col]).replace(\"\\\\\", os.sep)\n",
    "        p = os.path.join(self.root, rel) if self.root and not os.path.isabs(rel) else rel\n",
    "\n",
    "        im = Image.open(p).convert(\"L\")\n",
    "        if _HAS_ALBU:\n",
    "            arr = np.array(im)\n",
    "            arr3 = np.stack([arr, arr, arr], axis=2)  # (H, W, 3)\n",
    "            x = self.t(image=arr3)[\"image\"]\n",
    "        else:\n",
    "            x = self.t(im)\n",
    "\n",
    "        y = torch.tensor(int(row[self.label_col]), dtype=torch.float32)\n",
    "        meta = torch.tensor([\n",
    "            int(row[\"laterality_LEFT\"]),\n",
    "            int(row[\"laterality_RIGHT\"]),\n",
    "            int(row[\"view_CC\"]),\n",
    "            int(row[\"view_MLO\"]),\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        return x, y, meta\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\n",
    "train_ds = CropsDataset(train_df, transform=t_train)\n",
    "val_ds   = CropsDataset(val_df,   transform=t_val)\n",
    "\n",
    "# Weighted sampler for class imbalance\n",
    "class_counts = Counter(train_df[\"label\"].tolist())\n",
    "class_weights = {c: len(train_df)/class_counts[c] for c in class_counts}\n",
    "sample_weights = torch.tensor(train_df[\"label\"].map(class_weights).values, dtype=torch.float32)\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=(NUM_WORKERS > 0)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE*2, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=(NUM_WORKERS > 0)\n",
    ")\n",
    "\n",
    "xb, yb, mb = next(iter(train_loader))\n",
    "print(f\"OS: {'Windows' if IS_WIN else platform.system()} | num_workers={NUM_WORKERS} | pin_memory={PIN_MEMORY}\")\n",
    "print(\"Train batch:\", xb.shape, yb.shape, mb.shape, \"| pos in batch:\", int(yb.sum().item()))\n",
    "assert xb.ndim == 4 and xb.shape[1] == 3\n",
    "assert yb.ndim == 1 and mb.shape[1] == 4\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Cell 5 — Model Definition\n",
    "# ============================\n",
    "class ResNetWithMeta(nn.Module):\n",
    "    def __init__(self, meta_dim=4):\n",
    "        super().__init__()\n",
    "        base = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        # backbone up to (and including) avgpool, without fc\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])  # (B, 2048, 1, 1)\n",
    "        self.in_feat = base.fc.in_features  # 2048\n",
    "        self.meta_bn = nn.BatchNorm1d(meta_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.in_feat + meta_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, meta):\n",
    "        feats = self.backbone(x).squeeze(-1).squeeze(-1)  # (B, 2048)\n",
    "        meta = self.meta_bn(meta)                         # (B, 4)\n",
    "        fused = torch.cat([feats, meta], dim=1)           # (B, 2052)\n",
    "        return self.head(fused)                           # (B, 1)\n",
    "\n",
    "def make_model():\n",
    "    return ResNetWithMeta(meta_dim=4)\n",
    "\n",
    "model = make_model().to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(xb.to(DEVICE), mb.to(DEVICE))\n",
    "print(\"Model output shape:\", tuple(out.shape))\n",
    "assert out.shape[1] == 1\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Cell 6 — Loss, Optimizer, LR\n",
    "# ============================\n",
    "def focal_loss(inputs, targets, alpha=0.8, gamma=2.0, reduction='mean'):\n",
    "    bce = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    probs = torch.sigmoid(inputs)\n",
    "    p_t = probs*targets + (1-probs)*(1-targets)\n",
    "    loss = (alpha * (1-p_t)**gamma) * bce\n",
    "    return loss.mean() if reduction=='mean' else loss.sum()\n",
    "\n",
    "criterion_mix = lambda logits, y: 0.5*focal_loss(logits, y) + 0.5*nn.functional.binary_cross_entropy_with_logits(logits, y)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# tiny opt step sanity\n",
    "model.train()\n",
    "x_small = xb[:4].to(DEVICE); y_small = yb[:4].to(DEVICE).unsqueeze(1); m_small = mb[:4].to(DEVICE)\n",
    "optimizer.zero_grad()\n",
    "loss = criterion_mix(model(x_small, m_small), y_small)\n",
    "loss.backward(); optimizer.step()\n",
    "print(\"One tiny opt step OK. Loss:\", float(loss.item()))\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Cell 7 — Train & Evaluation Utils\n",
    "# ==================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    for x, y, m in loader:\n",
    "        x = x.to(DEVICE); m = m.to(DEVICE)\n",
    "        logits = model(x, m)\n",
    "        probs = torch.sigmoid(logits).squeeze(1).cpu().numpy().tolist()\n",
    "        y_prob += probs\n",
    "        y_true += y.numpy().tolist()\n",
    "    y_pred = [1 if p >= 0.5 else 0 for p in y_prob]\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob) if len(set(y_true)) > 1 else float(\"nan\")\n",
    "    return acc, f1, auc, (y_true, y_prob, y_pred)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for x, y, m in loader:\n",
    "        x = x.to(DEVICE); y = y.to(DEVICE).unsqueeze(1); m = m.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, m)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot += loss.item() * x.size(0)\n",
    "    return tot / len(loader.dataset)\n",
    "\n",
    "# Dry eval\n",
    "acc, f1, auc, _ = evaluate(model, val_loader)\n",
    "print(f\"Dry evaluation OK — acc={acc:.3f}, f1={f1:.3f}, auc={auc:.3f}\")\n",
    "\n",
    "# =======================\n",
    "# Cell 8 — Train the Model\n",
    "# =======================\n",
    "best_auc = -1.0\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss = train_epoch(model, train_loader, optimizer, criterion_mix)\n",
    "    acc, f1, auc, _ = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": tr_loss, \"val_acc\": acc, \"val_f1\": f1, \"val_auc\": auc})\n",
    "    print(f\"Epoch {epoch:02d}: loss={tr_loss:.4f} | acc={acc:.4f} | f1={f1:.4f} | auc={auc:.4f}\")\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"img_size\": IMG_SIZE,\n",
    "            \"mean\": IMAGENET_MEAN,\n",
    "            \"std\": IMAGENET_STD\n",
    "        }, OUT_PATH)\n",
    "\n",
    "print(\"Best AUC:\", best_auc, \"| Saved:\", OUT_PATH)\n",
    "assert os.path.exists(OUT_PATH), \"Checkpoint was not saved.\"\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Cell 9 — Final Validation & Threshold Tuning (Unified Style)\n",
    "# ==========================================\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report, matthews_corrcoef\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "ckpt = torch.load(OUT_PATH, map_location=DEVICE)\n",
    "model_best = make_model().to(DEVICE)\n",
    "model_best.load_state_dict(ckpt[\"model\"])\n",
    "model_best.eval()\n",
    "\n",
    "# Eval @0.5\n",
    "acc, f1, auc, (y_true, y_prob, y_pred) = evaluate(model_best, val_loader)\n",
    "print(f\"Best ckpt @0.5 — acc={acc:.4f}, f1={f1:.4f}, auc={auc:.4f}\")\n",
    "\n",
    "# Threshold sweep to maximize F1\n",
    "best_t, best_f1 = 0.5, f1\n",
    "for t in np.linspace(0.1, 0.9, 50):\n",
    "    yp = [1 if p >= t else 0 for p in y_prob]\n",
    "    f1_t = f1_score(y_true, yp)\n",
    "    if f1_t > best_f1:\n",
    "        best_f1, best_t = f1_t, t\n",
    "\n",
    "yp_opt = [1 if p >= best_t else 0 for p in y_prob]\n",
    "cm = confusion_matrix(y_true, yp_opt)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn + 1e-8)\n",
    "specificity = tn / (tn + fp + 1e-8)\n",
    "precision   = tp / (tp + fp + 1e-8)\n",
    "npv         = tn / (tn + fn + 1e-8)\n",
    "balanced_acc= 0.5 * (sensitivity + specificity)\n",
    "mcc         = matthews_corrcoef(y_true, yp_opt)\n",
    "\n",
    "print(\"\\n=== Metrics at optimal threshold ===\")\n",
    "print(f\"Threshold        : {best_t:.3f}\")\n",
    "print(f\"Accuracy         : {accuracy_score(y_true, yp_opt):.4f}\")\n",
    "print(f\"Balanced Acc     : {balanced_acc:.4f}\")\n",
    "print(f\"Sensitivity      : {sensitivity:.4f}\")\n",
    "print(f\"Specificity      : {specificity:.4f}\")\n",
    "print(f\"Precision        : {precision:.4f}\")\n",
    "print(f\"NPV              : {npv:.4f}\")\n",
    "print(f\"F1 Score         : {best_f1:.4f}\")\n",
    "print(f\"AUC              : {roc_auc_score(y_true, y_prob):.4f}\")\n",
    "print(f\"MCC              : {mcc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\",\n",
    "      classification_report(y_true, yp_opt, target_names=[\"benign\",\"malignant\"]))\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# Cell 10 — Grad-CAM Utilities & Example\n",
    "# =======================================\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "def preprocess_gray3(path, size=IMG_SIZE):\n",
    "    im = Image.open(path).convert(\"L\").resize((size,size))\n",
    "    im = np.array(im)\n",
    "    im = np.stack([im, im, im], axis=2)\n",
    "    t = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "    ])\n",
    "    x = t(Image.fromarray(im))\n",
    "    return x.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Works with the ResNetWithMeta wrapper. Hooks the 'layer4' block inside the backbone.\n",
    "    Since backbone is a Sequential of ResNet children (without fc), 'layer4' sits at index 7.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_idx=7):\n",
    "        self.model = model.eval()\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        layer = list(self.model.backbone.children())[target_layer_idx]\n",
    "        layer.register_forward_hook(self._fhook)\n",
    "        layer.register_full_backward_hook(self._bhook)\n",
    "\n",
    "    def _fhook(self, m, inp, out): self.activations = out.detach()\n",
    "    def _bhook(self, m, gin, gout): self.gradients = gout[0].detach()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x, meta):\n",
    "        logits = self.model(x, meta)\n",
    "        return torch.sigmoid(logits)[0,0].item()\n",
    "\n",
    "    def cam(self, x, meta):\n",
    "        logits = self.model(x, meta)\n",
    "        self.model.zero_grad()\n",
    "        logits[0,0].backward()\n",
    "        g = self.gradients\n",
    "        a = self.activations\n",
    "        w = torch.mean(g, dim=(2,3), keepdim=True)\n",
    "        cam = (w * a).sum(dim=1)\n",
    "        cam = F.relu(cam)[0].cpu().numpy()\n",
    "        cam = cv2.resize(cam, (x.shape[-1], x.shape[-2]))\n",
    "        cam = (cam - cam.min()) / (cam.max() + 1e-6)\n",
    "        return cam\n",
    "\n",
    "val_sample_path = os.path.join(ROOT_DIR, str(val_df.iloc[0][\"yolo_cropped_image_path\"]).replace(\"\\\\\", os.sep))\n",
    "x = preprocess_gray3(val_sample_path, size=IMG_SIZE)\n",
    "row0 = val_df.iloc[0]\n",
    "meta0 = torch.tensor([\n",
    "    float(row0[\"laterality_LEFT\"]), float(row0[\"laterality_RIGHT\"]),\n",
    "    float(row0[\"view_CC\"]), float(row0[\"view_MLO\"])\n",
    "], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "gc = GradCAM(model_best, target_layer_idx=7)\n",
    "prob = gc.predict(x, meta0)\n",
    "heat = gc.cam(x, meta0)\n",
    "print(\"Predicted malignant probability:\", prob)\n",
    "print(\"Grad-CAM heatmap shape:\", heat.shape, \"min/max:\", float(heat.min()), float(heat.max()))\n",
    "assert heat.ndim == 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mammo-xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
